{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW5XyW-KO0BC"
      },
      "source": [
        "# Naive RAG with Milvus and LangChain\n",
        "\n",
        "This notebook contains an implementation of RAG with Milvus, LangChain, and HuggingFace. Its purpose is to provide you with a starting point for coding, if required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJHRAig0O0BD"
      },
      "source": [
        "### Load (quantized) Phi-4 for Apple Sillicon hardware\n",
        "\n",
        "Using default `transformers` implementation is too slow on my MacBook (even though it is set to use `mps` device). Hence, I use the `mlx-lm` library. On `cuda` platforms, I recommend `unsloth`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain_milvus # TODO: Get rid of warning message"
      ],
      "metadata": {
        "id": "WnI3VYAP3jZu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain_community langchain_huggingface"
      ],
      "metadata": {
        "id": "ZuWxtJLP8mBY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8X6-2X2FPd6W"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "## Uncomment on CUDA platforms like Google Colab\n",
        "!pip install unsloth\n",
        "# # Also get the latest nightly Unsloth!\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "!pip install pymilvus[model]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXqasr4PuBEx",
        "outputId": "09d319ea-3e94-4587-8e81-f55c226329e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "30dbf64c3ce244059158c84b17562b14",
            "e75b587580004b9889ab62bc0cfebb0b",
            "dfb656bb9eb1447aba2cece0dfe53f4e",
            "85eb84e957d74b51b48cb8fe87a744e2",
            "8d6e5ca57d664f70843dbcdf868ca72b",
            "a76806d4ea0d427a90f682e012bfb44d",
            "8aea6319a4744ec292599c9188384792",
            "96d738d1f62c4f8c981671d104fe5a78",
            "5ce757801fda449681dd52e7d709c3de",
            "33f6848d9e544fe485ddcfc38fd26b21",
            "299b4a0b8b2744b48c7ffa8cb61bba13"
          ]
        },
        "id": "CEReoRyTO0BD",
        "outputId": "465113bc-d786-442d-8b0f-6af346bacf01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.1.7: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30dbf64c3ce244059158c84b17562b14"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    from mlx_lm import load\n",
        "\n",
        "    model, tokenizer = load(\n",
        "        \"mlx-community/phi-4-4bit\"\n",
        "    )  # <= replace with smaller model depending on WiFi bandwidth\n",
        "\n",
        "elif torch.cuda.is_available():\n",
        "    from unsloth import FastLanguageModel\n",
        "\n",
        "    model_name = \"unsloth/Phi-4-unsloth-bnb-4bit\"\n",
        "    max_seq_length = 2048\n",
        "    load_in_4bit = True\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/Phi-4\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        "    )\n",
        "\n",
        "else:\n",
        "    raise Exception(\n",
        "        \"You most likely don't have sufficient hardware to run this notebook... :(\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLv6H0rMO0BE"
      },
      "source": [
        "### Integration with LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "El7yh_4CQc1W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "njOJ5Y8oO0BE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdfa36ad-5c37-4f00-d41c-72273ba388e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    from langchain_community.llms.mlx_pipeline import MLXPipeline as Pipeline\n",
        "    from langchain_community.chat_models.mlx import ChatMLX as Chat\n",
        "\n",
        "    llm = Pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        pipeline_kwargs={\"max_tokens\": 1024, \"temp\": 0.1},\n",
        "    )\n",
        "\n",
        "elif torch.cuda.is_available():\n",
        "    import transformers\n",
        "    from langchain_huggingface import HuggingFacePipeline as Pipeline\n",
        "    from langchain_huggingface import ChatHuggingFace as Chat\n",
        "\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    hf_pipeline = transformers.pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        task=\"text-generation\",\n",
        "        # device=\"cuda\",\n",
        "        # repetition_penalty=1.15,\n",
        "        return_full_text=False,\n",
        "        max_new_tokens=1024,\n",
        "        # output_scores=True,\n",
        "        # use_cache=False,\n",
        "        # truncation=True\n",
        "    )\n",
        "\n",
        "    llm = Pipeline(pipeline=hf_pipeline)\n",
        "\n",
        "chat = Chat(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdUubzRHO0BE"
      },
      "source": [
        "### Test language model\n",
        "\n",
        "On Apple Silicon, ignore the warning, which is due to a breaking change in one of the libraries used in the past couple of weeks. That's why I pin `mlx-lm==0.20.6`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hgpc8QvVO0BE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8ca277-43d0-4cc0-c62e-cf50f988b5e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a language model, I cannot provide a definitive answer to the question of what happens when an unstoppable force meets an immovable object, as it is a classic paradox that challenges the definitions of \"unstoppable\" and \"immovable.\" This scenario is often used to explore concepts in philosophy, physics, and logic, highlighting the limitations of language and the nature of absolutes.\n",
            "\n",
            "In philosophy, this paradox is used to question the nature of reality and the limits of human understanding. In physics, it challenges the principles of motion and force, as the existence of both an unstoppable force and an immovable object simultaneously defies the laws of classical mechanics.\n",
            "\n",
            "Ultimately, the paradox serves as a thought experiment rather than a problem with a clear solution, encouraging deeper exploration of the concepts involved.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    HumanMessage(\n",
        "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "res = chat.invoke(messages)\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "# Replace with your GitHub repository details\n",
        "OWNER = \"microsoft\"  # e.g., \"octocat\"\n",
        "REPO = \"vscode\"  # e.g., \"hello-world\"\n",
        "\n",
        "\n",
        "# Base URL for GitHub API\n",
        "BASE_URL_ISSUES = f\"https://api.github.com/repos/{OWNER}/{REPO}/issues\"\n",
        "\n",
        "\n",
        "# Headers (include the token if accessing private repositories or to increase the rate limit)\n",
        "#HEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"} if GITHUB_TOKEN else {}\n",
        "\n",
        "\n",
        "def get_all_issues():\n",
        "   issues = []\n",
        "   page = 1  # GitHub paginates results (default 30 per page)\n",
        "\n",
        "\n",
        "   while page <= 100:\n",
        "       response = requests.get(BASE_URL_ISSUES, params={\"state\": \"all\", \"page\": page})\n",
        "       response.raise_for_status()  # Raise an error for bad HTTP status codes\n",
        "       data = response.json()\n",
        "       if not data:\n",
        "           break  # No more issues to fetch\n",
        "       issues.extend(data)\n",
        "       page += 1\n",
        "\n",
        "   return issues\n",
        "\n",
        "# Fetch issues\n",
        "all_issues = get_all_issues()\n"
      ],
      "metadata": {
        "id": "nxRPh9RSR-G_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "827be0bb-ff74-4ea6-bbe9-a82dedbe8f83"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "403 Client Error: rate limit exceeded for url: https://api.github.com/repos/microsoft/vscode/issues?state=all&page=61",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7b06e0c51215>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Fetch issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mall_issues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_issues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-7b06e0c51215>\u001b[0m in \u001b[0;36mget_all_issues\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m    \u001b[0;32mwhile\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m        \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_URL_ISSUES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"all\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"page\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m        \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Raise an error for bad HTTP status codes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m        \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: rate limit exceeded for url: https://api.github.com/repos/microsoft/vscode/issues?state=all&page=61"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(all_issues)\n",
        "#for issue in all_issues:\n",
        "#  print(\"keys\")\n",
        "#  print(issue.keys())\n",
        "#  print(issue['title'])\n",
        "#  print(issue['body'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a8JSajBBSjDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "def split_issues(all_issues):\n",
        " all_docs = []\n",
        " for issue in all_issues:\n",
        "   try:\n",
        "     text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200, length_function=len,\n",
        "     is_separator_regex=False)\n",
        "     concatenated = issue['title'] + ' ' + issue['body']\n",
        "     texts = text_splitter.create_documents([concatenated])\n",
        "     docs = text_splitter.split_documents(texts)\n",
        "\n",
        "     all_docs.extend(docs)\n",
        "   except:\n",
        "     pass\n",
        "     #print(f'skipped issue')\n",
        " return all_docs\n",
        "\n",
        "\n",
        "issue_chunks = split_issues(all_issues)"
      ],
      "metadata": {
        "id": "89_AMV7_si7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [chunk.page_content for chunk in issue_chunks]\n",
        "print(len(docs))"
      ],
      "metadata": {
        "id": "-ikVo06wvtld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pymilvus import model\n",
        "\n",
        "# # If connection to https://huggingface.co/ failed, uncomment the following path\n",
        "# # import os\n",
        "# # os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
        "\n",
        "# # This will download a small embedding model \"paraphrase-albert-small-v2\" (~50MB).\n",
        "# embedding_fn = model.DefaultEmbeddingFunction()\n",
        "\n",
        "# # Text strings to search from.\n",
        "# # docs = [\n",
        "# #     \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
        "# #     \"Alan Turing was the first person to conduct substantial research in AI.\",\n",
        "# #     \"Born in Maida Vale, London, Turing was raised in southern England.\",\n",
        "# # ]\n",
        "\n",
        "# vectors = embedding_fn.encode_documents(docs)\n",
        "# # The output vector has 768 dimensions, matching the collection that we just created.\n",
        "# print(\"Dim:\", embedding_fn.dim, vectors[0].shape)  # Dim: 768 (768,)\n",
        "\n",
        "# # Each entity has id, vector representation, raw text, and a subject label that we use\n",
        "# # to demo metadata filtering later.\n",
        "# data = [\n",
        "#     {\"id\": i, \"vector\": vectors[i], \"text\": docs[i], \"subject\": \"history\"}\n",
        "#     for i in range(len(vectors))\n",
        "# ]\n",
        "\n",
        "# print(\"Data has\", len(data), \"entities, each with fields: \", data[0].keys())\n",
        "# print(\"Vector dim:\", len(data[0][\"vector\"]))"
      ],
      "metadata": {
        "id": "oU47LRkgXp-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlM5gqyiO0BE"
      },
      "source": [
        "### Prepare the Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzbDiCT2O0BF"
      },
      "source": [
        "### Build naive RAG with Milvus and LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-7CTsGiO0BF"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "embeddings = SentenceTransformerEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt22hMBTO0BF"
      },
      "outputs": [],
      "source": [
        "from langchain_milvus import Milvus, Zilliz\n",
        "\n",
        "vectorstore = Milvus.from_documents(  # or Zilliz.from_documents\n",
        "    documents=issue_chunks,\n",
        "    embedding=embeddings,\n",
        "    connection_args={\n",
        "        \"uri\": \"./milvus_demo.db\",\n",
        "    },\n",
        "    drop_old=True,  # Drop the old Milvus collection if it exists\n",
        "    index_params={\n",
        "        \"metric_type\": \"COSINE\",\n",
        "        \"index_type\": \"FLAT\",  # <= NOTE: Currently a bug where langchain_milvus defaults to \"HNSW\" index, which doesn't work with Milvus Lite\n",
        "        \"params\": {},\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc9R0cneO0BF"
      },
      "source": [
        "### Test vector database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKBoN8_yO0BF"
      },
      "outputs": [],
      "source": [
        "query = \"Why is my UI slow?\"\n",
        "res = vectorstore.similarity_search(query, k=1)\n",
        "print(res[0].page_content[0:1024] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrXWLrcOO0BF"
      },
      "source": [
        "### Extra LangChain stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSYUgcTtO0BF"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define the prompt template for generating AI responses\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Human: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\n",
        "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "The response should be specific and use statistics or numbers when possible.\n",
        "\n",
        "Assistant:\"\"\"\n",
        "\n",
        "# Create a PromptTemplate instance with the defined template and input variables\n",
        "prompt = PromptTemplate(\n",
        "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "# Convert the vector store to a retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "# Define a function to format the retrieved documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASbXqNXWO0BF"
      },
      "source": [
        "### LangChain Expression Language\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W08c9qlcO0BF"
      },
      "outputs": [],
      "source": [
        "# Define the RAG (Retrieval-Augmented Generation) chain for AI response generation\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# rag_chain.get_graph().print_ascii()\n",
        "\n",
        "# Invoke the RAG chain with a specific question and retrieve the response\n",
        "res = rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Jj-pY4NO0BG"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "# TODO: Better text wrapping in Colab\n",
        "print(textwrap.fill(res, width=80, replace_whitespace=False, drop_whitespace=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-hoCG9vO0BG"
      },
      "source": [
        "### You have successfully built and run a RAG pipeline using Milvus, Hugging Face, and LangChain libraries!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx36kWrpO0BG"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30dbf64c3ce244059158c84b17562b14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e75b587580004b9889ab62bc0cfebb0b",
              "IPY_MODEL_dfb656bb9eb1447aba2cece0dfe53f4e",
              "IPY_MODEL_85eb84e957d74b51b48cb8fe87a744e2"
            ],
            "layout": "IPY_MODEL_8d6e5ca57d664f70843dbcdf868ca72b"
          }
        },
        "e75b587580004b9889ab62bc0cfebb0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a76806d4ea0d427a90f682e012bfb44d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8aea6319a4744ec292599c9188384792",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "dfb656bb9eb1447aba2cece0dfe53f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96d738d1f62c4f8c981671d104fe5a78",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ce757801fda449681dd52e7d709c3de",
            "value": 3
          }
        },
        "85eb84e957d74b51b48cb8fe87a744e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33f6848d9e544fe485ddcfc38fd26b21",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_299b4a0b8b2744b48c7ffa8cb61bba13",
            "value": "â€‡3/3â€‡[00:47&lt;00:00,â€‡14.24s/it]"
          }
        },
        "8d6e5ca57d664f70843dbcdf868ca72b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a76806d4ea0d427a90f682e012bfb44d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aea6319a4744ec292599c9188384792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96d738d1f62c4f8c981671d104fe5a78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ce757801fda449681dd52e7d709c3de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33f6848d9e544fe485ddcfc38fd26b21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "299b4a0b8b2744b48c7ffa8cb61bba13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}