{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW5XyW-KO0BC"
      },
      "source": [
        "# Naive RAG with Milvus and LangChain\n",
        "\n",
        "This notebook contains an implementation of RAG with Milvus, LangChain, and HuggingFace. Its purpose is to provide you with a starting point for coding, if required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJHRAig0O0BD"
      },
      "source": [
        "### Load (quantized) Phi-4 for Apple Sillicon hardware\n",
        "\n",
        "Using default `transformers` implementation is too slow on my MacBook (even though it is set to use `mps` device). Hence, I use the `mlx-lm` library. On `cuda` platforms, I recommend `unsloth`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain_milvus # TODO: Get rid of warning message"
      ],
      "metadata": {
        "id": "WnI3VYAP3jZu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain_community langchain_huggingface"
      ],
      "metadata": {
        "id": "ZuWxtJLP8mBY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X6-2X2FPd6W"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "## Uncomment on CUDA platforms like Google Colab\n",
        "!pip install unsloth\n",
        "# # Also get the latest nightly Unsloth!\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "!pip install pymilvus[model]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "id": "CXqasr4PuBEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEReoRyTO0BD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    from mlx_lm import load\n",
        "\n",
        "    model, tokenizer = load(\n",
        "        \"mlx-community/phi-4-4bit\"\n",
        "    )  # <= replace with smaller model depending on WiFi bandwidth\n",
        "\n",
        "elif torch.cuda.is_available():\n",
        "    from unsloth import FastLanguageModel\n",
        "\n",
        "    model_name = \"unsloth/Phi-4-unsloth-bnb-4bit\"\n",
        "    max_seq_length = 2048\n",
        "    load_in_4bit = True\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/Phi-4\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        load_in_4bit=load_in_4bit,\n",
        "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        "    )\n",
        "\n",
        "else:\n",
        "    raise Exception(\n",
        "        \"You most likely don't have sufficient hardware to run this notebook... :(\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLv6H0rMO0BE"
      },
      "source": [
        "### Integration with LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El7yh_4CQc1W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njOJ5Y8oO0BE"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    from langchain_community.llms.mlx_pipeline import MLXPipeline as Pipeline\n",
        "    from langchain_community.chat_models.mlx import ChatMLX as Chat\n",
        "\n",
        "    llm = Pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        pipeline_kwargs={\"max_tokens\": 1024, \"temp\": 0.1},\n",
        "    )\n",
        "\n",
        "elif torch.cuda.is_available():\n",
        "    import transformers\n",
        "    from langchain_huggingface import HuggingFacePipeline as Pipeline\n",
        "    from langchain_huggingface import ChatHuggingFace as Chat\n",
        "\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    hf_pipeline = transformers.pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        task=\"text-generation\",\n",
        "        # device=\"cuda\",\n",
        "        # repetition_penalty=1.15,\n",
        "        return_full_text=False,\n",
        "        max_new_tokens=1024,\n",
        "        # output_scores=True,\n",
        "        # use_cache=False,\n",
        "        # truncation=True\n",
        "    )\n",
        "\n",
        "    llm = Pipeline(pipeline=hf_pipeline)\n",
        "\n",
        "chat = Chat(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdUubzRHO0BE"
      },
      "source": [
        "### Test language model\n",
        "\n",
        "On Apple Silicon, ignore the warning, which is due to a breaking change in one of the libraries used in the past couple of weeks. That's why I pin `mlx-lm==0.20.6`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgpc8QvVO0BE"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    HumanMessage(\n",
        "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "res = chat.invoke(messages)\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "# Replace with your GitHub repository details\n",
        "OWNER = \"microsoft\"  # e.g., \"octocat\"\n",
        "REPO = \"vscode\"  # e.g., \"hello-world\"\n",
        "\n",
        "\n",
        "# Base URL for GitHub API\n",
        "BASE_URL_ISSUES = f\"https://api.github.com/repos/{OWNER}/{REPO}/issues\"\n",
        "\n",
        "\n",
        "# Headers (include the token if accessing private repositories or to increase the rate limit)\n",
        "#HEADERS = {\"Authorization\": f\"token {GITHUB_TOKEN}\"} if GITHUB_TOKEN else {}\n",
        "\n",
        "\n",
        "def get_all_issues():\n",
        "   issues = []\n",
        "   page = 1  # GitHub paginates results (default 30 per page)\n",
        "\n",
        "\n",
        "   while page <= 100:\n",
        "       response = requests.get(BASE_URL_ISSUES, params={\"state\": \"all\", \"page\": page})\n",
        "       response.raise_for_status()  # Raise an error for bad HTTP status codes\n",
        "       data = response.json()\n",
        "       if not data:\n",
        "           break  # No more issues to fetch\n",
        "       issues.extend(data)\n",
        "       page += 1\n",
        "\n",
        "   return issues\n",
        "\n",
        "# Fetch issues\n",
        "all_issues = get_all_issues()\n"
      ],
      "metadata": {
        "id": "nxRPh9RSR-G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(all_issues)\n",
        "#for issue in all_issues:\n",
        "#  print(\"keys\")\n",
        "#  print(issue.keys())\n",
        "#  print(issue['title'])\n",
        "#  print(issue['body'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a8JSajBBSjDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "def split_issues(all_issues):\n",
        " all_docs = []\n",
        " for issue in all_issues:\n",
        "   try:\n",
        "     text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200, length_function=len,\n",
        "     is_separator_regex=False)\n",
        "     concatenated = issue['title'] + ' ' + issue['body']\n",
        "     texts = text_splitter.create_documents([concatenated])\n",
        "     docs = text_splitter.split_documents(texts)\n",
        "\n",
        "     all_docs.extend(docs)\n",
        "   except:\n",
        "     pass\n",
        "     #print(f'skipped issue')\n",
        " return all_docs\n",
        "\n",
        "\n",
        "issue_chunks = split_issues(all_issues)"
      ],
      "metadata": {
        "id": "89_AMV7_si7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [chunk.page_content for chunk in issue_chunks]\n",
        "print(len(docs))"
      ],
      "metadata": {
        "id": "-ikVo06wvtld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pymilvus import model\n",
        "\n",
        "# # If connection to https://huggingface.co/ failed, uncomment the following path\n",
        "# # import os\n",
        "# # os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
        "\n",
        "# # This will download a small embedding model \"paraphrase-albert-small-v2\" (~50MB).\n",
        "# embedding_fn = model.DefaultEmbeddingFunction()\n",
        "\n",
        "# # Text strings to search from.\n",
        "# # docs = [\n",
        "# #     \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
        "# #     \"Alan Turing was the first person to conduct substantial research in AI.\",\n",
        "# #     \"Born in Maida Vale, London, Turing was raised in southern England.\",\n",
        "# # ]\n",
        "\n",
        "# vectors = embedding_fn.encode_documents(docs)\n",
        "# # The output vector has 768 dimensions, matching the collection that we just created.\n",
        "# print(\"Dim:\", embedding_fn.dim, vectors[0].shape)  # Dim: 768 (768,)\n",
        "\n",
        "# # Each entity has id, vector representation, raw text, and a subject label that we use\n",
        "# # to demo metadata filtering later.\n",
        "# data = [\n",
        "#     {\"id\": i, \"vector\": vectors[i], \"text\": docs[i], \"subject\": \"history\"}\n",
        "#     for i in range(len(vectors))\n",
        "# ]\n",
        "\n",
        "# print(\"Data has\", len(data), \"entities, each with fields: \", data[0].keys())\n",
        "# print(\"Vector dim:\", len(data[0][\"vector\"]))"
      ],
      "metadata": {
        "id": "oU47LRkgXp-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlM5gqyiO0BE"
      },
      "source": [
        "### Prepare the Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzbDiCT2O0BF"
      },
      "source": [
        "### Build naive RAG with Milvus and LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-7CTsGiO0BF"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "embeddings = SentenceTransformerEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt22hMBTO0BF"
      },
      "outputs": [],
      "source": [
        "from langchain_milvus import Milvus, Zilliz\n",
        "\n",
        "vectorstore = Milvus.from_documents(  # or Zilliz.from_documents\n",
        "    documents=issue_chunks,\n",
        "    embedding=embeddings,\n",
        "    connection_args={\n",
        "        \"uri\": \"./milvus_demo.db\",\n",
        "    },\n",
        "    drop_old=True,  # Drop the old Milvus collection if it exists\n",
        "    index_params={\n",
        "        \"metric_type\": \"COSINE\",\n",
        "        \"index_type\": \"FLAT\",  # <= NOTE: Currently a bug where langchain_milvus defaults to \"HNSW\" index, which doesn't work with Milvus Lite\n",
        "        \"params\": {},\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc9R0cneO0BF"
      },
      "source": [
        "### Test vector database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKBoN8_yO0BF"
      },
      "outputs": [],
      "source": [
        "query = \"Why is my UI slow?\"\n",
        "res = vectorstore.similarity_search(query, k=1)\n",
        "print(res[0].page_content[0:1024] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrXWLrcOO0BF"
      },
      "source": [
        "### Extra LangChain stuff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSYUgcTtO0BF"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define the prompt template for generating AI responses\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Human: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\n",
        "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\n",
        "The response should be specific and use statistics or numbers when possible.\n",
        "\n",
        "Assistant:\"\"\"\n",
        "\n",
        "# Create a PromptTemplate instance with the defined template and input variables\n",
        "prompt = PromptTemplate(\n",
        "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "# Convert the vector store to a retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "# Define a function to format the retrieved documents\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASbXqNXWO0BF"
      },
      "source": [
        "### LangChain Expression Language\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W08c9qlcO0BF"
      },
      "outputs": [],
      "source": [
        "# Define the RAG (Retrieval-Augmented Generation) chain for AI response generation\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# rag_chain.get_graph().print_ascii()\n",
        "\n",
        "# Invoke the RAG chain with a specific question and retrieve the response\n",
        "res = rag_chain.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Jj-pY4NO0BG"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "# TODO: Better text wrapping in Colab\n",
        "print(textwrap.fill(res, width=80, replace_whitespace=False, drop_whitespace=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-hoCG9vO0BG"
      },
      "source": [
        "### You have successfully built and run a RAG pipeline using Milvus, Hugging Face, and LangChain libraries!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx36kWrpO0BG"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}